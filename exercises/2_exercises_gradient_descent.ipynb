{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e1179fa-80d6-4540-b246-419a435b5375",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b11ac6b-4f4e-426c-9a58-bdd913508990",
   "metadata": {},
   "source": [
    "## 2.7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f2ebd9-bd9c-40ba-8c7c-7d2868435b05",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B.\n401 Client Error. (Request ID: Root=1-69138265-112ab56f388542320fb8480e;c17c3795-c8e1-4c58-8de5-8e4416eb18f8)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-1B is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    404\u001b[0m         path_or_repo_id,\n\u001b[1;32m    405\u001b[0m         filename,\n\u001b[1;32m    406\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    407\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    408\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    409\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    410\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    411\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    412\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    413\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    414\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    415\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m    863\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    864\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[1;32m    866\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m    867\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m    868\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    869\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[1;32m    871\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m    872\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m    873\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    874\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    875\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[1;32m    877\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    878\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    879\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1484\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1483\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1486\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1376\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1377\u001b[0m         url\u001b[38;5;241m=\u001b[39murl, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout, headers\u001b[38;5;241m=\u001b[39mheaders, token\u001b[38;5;241m=\u001b[39mtoken\n\u001b[1;32m   1378\u001b[0m     )\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1297\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1298\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   1299\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1300\u001b[0m     allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1301\u001b[0m     follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1302\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1303\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1304\u001b[0m )\n\u001b[1;32m   1305\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:277\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 277\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m    278\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    279\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    280\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    300\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 301\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:423\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    420\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    422\u001b[0m     )\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-69138265-112ab56f388542320fb8480e;c17c3795-c8e1-4c58-8de5-8e4416eb18f8)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-1B is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-1B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:526\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 526\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    527\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    528\u001b[0m     return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    529\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m    530\u001b[0m     code_revision\u001b[38;5;241m=\u001b[39mcode_revision,\n\u001b[1;32m    531\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    534\u001b[0m )\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1017\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1015\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1017\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1018\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1019\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py:574\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 574\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py:633\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m    634\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    635\u001b[0m         configuration_file,\n\u001b[1;32m    636\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    637\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    638\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    639\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    640\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    641\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    642\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    643\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    644\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m    645\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B.\n401 Client Error. (Request ID: Root=1-69138265-112ab56f388542320fb8480e;c17c3795-c8e1-4c58-8de5-8e4416eb18f8)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-1B is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device='cuda'\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9a221-19de-4edc-a0b0-c1a065693658",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The capital of France is Paris\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "outputs.loss  # This is the average cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8d712-aaf8-4588-a7c5-28d76d8564f8",
   "metadata": {},
   "source": [
    "Walk through loss computation in a nice table, then remove parts for exercise in book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a87f7-d268-432f-a528-f3035ab77b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba22e62-fb54-434a-a129-23614f2288be",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits  # Shape: (1, sequence_length, vocab_size)\n",
    "\n",
    "# Get probabilities\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Prepare data for DataFrame\n",
    "data = []\n",
    "token_ids = inputs[\"input_ids\"][0].cpu().numpy()\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "for i in range(len(token_ids) - 1):\n",
    "    # Input text up to this point\n",
    "    input_so_far = tokenizer.decode(token_ids[:i+1])\n",
    "    \n",
    "    # Get probabilities for next token prediction\n",
    "    next_token_probs = probs[0, i, :]\n",
    "    \n",
    "    # Most likely next token\n",
    "    most_likely_token_id = torch.argmax(next_token_probs).item()\n",
    "    most_likely_token = tokenizer.decode([most_likely_token_id])\n",
    "    most_likely_prob = next_token_probs[most_likely_token_id].item()\n",
    "    \n",
    "    # Correct next token (actual next token in sequence)\n",
    "    correct_token_id = token_ids[i + 1]\n",
    "    correct_token = tokenizer.decode([correct_token_id])\n",
    "    correct_prob = next_token_probs[correct_token_id].item()\n",
    "    \n",
    "    # Negative log likelihood (cross-entropy loss for this token)\n",
    "    nll = -torch.log(next_token_probs[correct_token_id]).item()\n",
    "    \n",
    "    data.append({\n",
    "        'Input Text So Far': input_so_far,\n",
    "        'Most Likely Next Token': most_likely_token,\n",
    "        'Prob of Most Likely': f\"{most_likely_prob:.6f}\",\n",
    "        'Correct Next Token': correct_token,\n",
    "        'Prob of Correct Token': f\"{correct_prob:.6f}\",\n",
    "        'Negative Log Prob': f\"{nll:.6f}\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('/home/stephen/book_exports/exercise_27.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dde6324-856d-45e5-9762-a127fd642011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Text So Far</th>\n",
       "      <th>Most Likely Next Token</th>\n",
       "      <th>Prob of Most Likely</th>\n",
       "      <th>Correct Next Token</th>\n",
       "      <th>Prob of Correct Token</th>\n",
       "      <th>Negative Log Prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|begin_of_text|&gt;</td>\n",
       "      <td>Question</td>\n",
       "      <td>0.3013</td>\n",
       "      <td>The</td>\n",
       "      <td>0.0267</td>\n",
       "      <td>3.6222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|begin_of_text|&gt;The</td>\n",
       "      <td></td>\n",
       "      <td>0.0244</td>\n",
       "      <td>capital</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>8.6831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;|begin_of_text|&gt;The capital</td>\n",
       "      <td>of</td>\n",
       "      <td>0.5687</td>\n",
       "      <td>of</td>\n",
       "      <td>0.5687</td>\n",
       "      <td>0.5645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;|begin_of_text|&gt;The capital of</td>\n",
       "      <td>the</td>\n",
       "      <td>0.2047</td>\n",
       "      <td>France</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>4.4855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;|begin_of_text|&gt;The capital of France</td>\n",
       "      <td>,</td>\n",
       "      <td>0.5081</td>\n",
       "      <td>is</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>1.9581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;|begin_of_text|&gt;The capital of France is</td>\n",
       "      <td>Paris</td>\n",
       "      <td>0.3915</td>\n",
       "      <td>Paris</td>\n",
       "      <td>0.3915</td>\n",
       "      <td>0.9377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Input Text So Far Most Likely Next Token  \\\n",
       "0                          <|begin_of_text|>               Question   \n",
       "1                       <|begin_of_text|>The                          \n",
       "2               <|begin_of_text|>The capital                     of   \n",
       "3            <|begin_of_text|>The capital of                    the   \n",
       "4     <|begin_of_text|>The capital of France                      ,   \n",
       "5  <|begin_of_text|>The capital of France is                  Paris   \n",
       "\n",
       "  Prob of Most Likely Correct Next Token Prob of Correct Token  \\\n",
       "0              0.3013                The                0.0267   \n",
       "1              0.0244            capital                0.0002   \n",
       "2              0.5687                 of                0.5687   \n",
       "3              0.2047             France                0.0113   \n",
       "4              0.5081                 is                0.1411   \n",
       "5              0.3915              Paris                0.3915   \n",
       "\n",
       "  Negative Log Prob  \n",
       "0            3.6222  \n",
       "1            8.6831  \n",
       "2            0.5645  \n",
       "3            4.4855  \n",
       "4            1.9581  \n",
       "5            0.9377  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf83a3a6-1599-494f-bdf7-d6a43d0854b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.3751833333333336)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_numeric(df['Negative Log Prob']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1ae11-1df0-4750-8450-05fe022f8843",
   "metadata": {},
   "source": [
    "## 2.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c538c439-c86e-4892-81ba-6ea686dca2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device='cuda'\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49c9920-254f-408f-928e-4e1ac52b628d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9323, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"An apple a day keeps the doctor away\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "outputs.loss  # This is the average cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "262066a5-f846-4da7-85cf-a5d33d8c9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits  # Shape: (1, sequence_length, vocab_size)\n",
    "\n",
    "# Get probabilities\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Prepare data for DataFrame\n",
    "data = []\n",
    "token_ids = inputs[\"input_ids\"][0].cpu().numpy()\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "for i in range(len(token_ids) - 1):\n",
    "    # Input text up to this point\n",
    "    input_so_far = tokenizer.decode(token_ids[:i+1])\n",
    "    \n",
    "    # Get probabilities for next token prediction\n",
    "    next_token_probs = probs[0, i, :]\n",
    "    \n",
    "    # Most likely next token\n",
    "    most_likely_token_id = torch.argmax(next_token_probs).item()\n",
    "    most_likely_token = tokenizer.decode([most_likely_token_id])\n",
    "    most_likely_prob = next_token_probs[most_likely_token_id].item()\n",
    "    \n",
    "    # Correct next token (actual next token in sequence)\n",
    "    correct_token_id = token_ids[i + 1]\n",
    "    correct_token = tokenizer.decode([correct_token_id])\n",
    "    correct_prob = next_token_probs[correct_token_id].item()\n",
    "    \n",
    "    # Negative log likelihood (cross-entropy loss for this token)\n",
    "    nll = -torch.log(next_token_probs[correct_token_id]).item()\n",
    "    \n",
    "    data.append({\n",
    "        'Input Text So Far': input_so_far,\n",
    "        'Most Likely Next Token': most_likely_token,\n",
    "        'Prob of Most Likely': f\"{most_likely_prob:.4f}\",\n",
    "        'Correct Next Token': correct_token,\n",
    "        'Prob of Correct Token': f\"{correct_prob:.4f}\",\n",
    "        'Negative Log Prob': f\"{nll:.4f}\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('/home/stephen/book_exports/exercise_212.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "544dd00d-a43f-4c44-91f3-cd6a694bb326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Text So Far</th>\n",
       "      <th>Most Likely Next Token</th>\n",
       "      <th>Prob of Most Likely</th>\n",
       "      <th>Correct Next Token</th>\n",
       "      <th>Prob of Correct Token</th>\n",
       "      <th>Negative Log Prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|begin_of_text|&gt;</td>\n",
       "      <td>Question</td>\n",
       "      <td>0.3013</td>\n",
       "      <td>An</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>6.3816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|begin_of_text|&gt;An</td>\n",
       "      <td></td>\n",
       "      <td>0.0228</td>\n",
       "      <td>apple</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>7.3936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;|begin_of_text|&gt;An apple</td>\n",
       "      <td>a</td>\n",
       "      <td>0.6497</td>\n",
       "      <td>a</td>\n",
       "      <td>0.6497</td>\n",
       "      <td>0.4312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;|begin_of_text|&gt;An apple a</td>\n",
       "      <td>day</td>\n",
       "      <td>0.9866</td>\n",
       "      <td>day</td>\n",
       "      <td>0.9866</td>\n",
       "      <td>0.0135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;|begin_of_text|&gt;An apple a day</td>\n",
       "      <td>keeps</td>\n",
       "      <td>0.4835</td>\n",
       "      <td>keeps</td>\n",
       "      <td>0.4835</td>\n",
       "      <td>0.7267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;|begin_of_text|&gt;An apple a day keeps</td>\n",
       "      <td>the</td>\n",
       "      <td>0.8508</td>\n",
       "      <td>the</td>\n",
       "      <td>0.8508</td>\n",
       "      <td>0.1616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;|begin_of_text|&gt;An apple a day keeps the</td>\n",
       "      <td>doctor</td>\n",
       "      <td>0.7380</td>\n",
       "      <td>doctor</td>\n",
       "      <td>0.7380</td>\n",
       "      <td>0.3038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;|begin_of_text|&gt;An apple a day keeps the doctor</td>\n",
       "      <td>away</td>\n",
       "      <td>0.9548</td>\n",
       "      <td>away</td>\n",
       "      <td>0.9548</td>\n",
       "      <td>0.0462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Input Text So Far Most Likely Next Token  \\\n",
       "0                                 <|begin_of_text|>               Question   \n",
       "1                               <|begin_of_text|>An                          \n",
       "2                         <|begin_of_text|>An apple                      a   \n",
       "3                       <|begin_of_text|>An apple a                    day   \n",
       "4                   <|begin_of_text|>An apple a day                  keeps   \n",
       "5             <|begin_of_text|>An apple a day keeps                    the   \n",
       "6         <|begin_of_text|>An apple a day keeps the                 doctor   \n",
       "7  <|begin_of_text|>An apple a day keeps the doctor                   away   \n",
       "\n",
       "  Prob of Most Likely Correct Next Token Prob of Correct Token  \\\n",
       "0              0.3013                 An                0.0017   \n",
       "1              0.0228              apple                0.0006   \n",
       "2              0.6497                  a                0.6497   \n",
       "3              0.9866                day                0.9866   \n",
       "4              0.4835              keeps                0.4835   \n",
       "5              0.8508                the                0.8508   \n",
       "6              0.7380             doctor                0.7380   \n",
       "7              0.9548               away                0.9548   \n",
       "\n",
       "  Negative Log Prob  \n",
       "0            6.3816  \n",
       "1            7.3936  \n",
       "2            0.4312  \n",
       "3            0.0135  \n",
       "4            0.7267  \n",
       "5            0.1616  \n",
       "6            0.3038  \n",
       "7            0.0462  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "389eb73e-e5eb-4a0a-bd09-447d855e1c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.932275)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_numeric(df['Negative Log Prob']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b51d8e-6d7f-4430-ae84-0b2cbdbe9a30",
   "metadata": {},
   "source": [
    "## 2.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fa69f4c-4d1b-4468-841e-cc7fb98cc300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device='cuda'\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "587a7f88-f98d-4cd8-bb06-24e1f9a4ca4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5667, device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I've had a perfectly wonderful evening, but this wasn't it\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "outputs.loss  # This is the average cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "270cf9cd-ebed-41a1-b008-ad395e8e0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits  # Shape: (1, sequence_length, vocab_size)\n",
    "\n",
    "# Get probabilities\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Prepare data for DataFrame\n",
    "data = []\n",
    "token_ids = inputs[\"input_ids\"][0].cpu().numpy()\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "for i in range(len(token_ids) - 1):\n",
    "    # Input text up to this point\n",
    "    input_so_far = tokenizer.decode(token_ids[:i+1])\n",
    "    \n",
    "    # Get probabilities for next token prediction\n",
    "    next_token_probs = probs[0, i, :]\n",
    "    \n",
    "    # Most likely next token\n",
    "    most_likely_token_id = torch.argmax(next_token_probs).item()\n",
    "    most_likely_token = tokenizer.decode([most_likely_token_id])\n",
    "    most_likely_prob = next_token_probs[most_likely_token_id].item()\n",
    "    \n",
    "    # Correct next token (actual next token in sequence)\n",
    "    correct_token_id = token_ids[i + 1]\n",
    "    correct_token = tokenizer.decode([correct_token_id])\n",
    "    correct_prob = next_token_probs[correct_token_id].item()\n",
    "    \n",
    "    # Negative log likelihood (cross-entropy loss for this token)\n",
    "    nll = -torch.log(next_token_probs[correct_token_id]).item()\n",
    "    \n",
    "    data.append({\n",
    "        'Input Text So Far': input_so_far,\n",
    "        'Most Likely Next Token': most_likely_token,\n",
    "        'Prob of Most Likely': f\"{most_likely_prob:.4f}\",\n",
    "        'Correct Next Token': correct_token,\n",
    "        'Prob of Correct Token': f\"{correct_prob:.4f}\",\n",
    "        'Negative Log Prob': f\"{nll:.4f}\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('/home/stephen/book_exports/exercise_217.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ab8a461-4bd2-45c7-ab03-b466b999dde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Text So Far</th>\n",
       "      <th>Most Likely Next Token</th>\n",
       "      <th>Prob of Most Likely</th>\n",
       "      <th>Correct Next Token</th>\n",
       "      <th>Prob of Correct Token</th>\n",
       "      <th>Negative Log Prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|begin_of_text|&gt;</td>\n",
       "      <td>Question</td>\n",
       "      <td>0.3013</td>\n",
       "      <td>I</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>4.9784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|begin_of_text|&gt;I</td>\n",
       "      <td>have</td>\n",
       "      <td>0.0934</td>\n",
       "      <td>'ve</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>3.5150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;|begin_of_text|&gt;I've</td>\n",
       "      <td>been</td>\n",
       "      <td>0.3544</td>\n",
       "      <td>had</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>3.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;|begin_of_text|&gt;I've had</td>\n",
       "      <td>a</td>\n",
       "      <td>0.3027</td>\n",
       "      <td>a</td>\n",
       "      <td>0.3027</td>\n",
       "      <td>1.1951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;|begin_of_text|&gt;I've had a</td>\n",
       "      <td>few</td>\n",
       "      <td>0.1439</td>\n",
       "      <td>perfectly</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>9.2520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;|begin_of_text|&gt;I've had a perfectly</td>\n",
       "      <td>good</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>3.2614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;|begin_of_text|&gt;I've had a perfectly wonderful</td>\n",
       "      <td>life</td>\n",
       "      <td>0.1588</td>\n",
       "      <td>evening</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>4.6765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;|begin_of_text|&gt;I've had a perfectly wonderfu...</td>\n",
       "      <td>with</td>\n",
       "      <td>0.1649</td>\n",
       "      <td>,</td>\n",
       "      <td>0.1101</td>\n",
       "      <td>2.2059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;|begin_of_text|&gt;I've had a perfectly wonderfu...</td>\n",
       "      <td>and</td>\n",
       "      <td>0.1038</td>\n",
       "      <td>but</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>2.2735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;|begin_of_text|&gt;I've had a perfectly wonderfu...</td>\n",
       "      <td>I</td>\n",
       "      <td>0.3272</td>\n",
       "      <td>this</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>4.0239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;|begin_of_text|&gt;I've had a perfectly wonderfu...</td>\n",
       "      <td>morning</td>\n",
       "      <td>0.4198</td>\n",
       "      <td>wasn</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>6.4166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;|begin_of_text|&gt;I've had a perfectly wonderfu...</td>\n",
       "      <td>'t</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>'t</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;|begin_of_text|&gt;I've had a perfectly wonderfu...</td>\n",
       "      <td>it</td>\n",
       "      <td>0.2101</td>\n",
       "      <td>it</td>\n",
       "      <td>0.2101</td>\n",
       "      <td>1.5600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Input Text So Far Most Likely Next Token  \\\n",
       "0                                   <|begin_of_text|>               Question   \n",
       "1                                  <|begin_of_text|>I                   have   \n",
       "2                               <|begin_of_text|>I've                   been   \n",
       "3                           <|begin_of_text|>I've had                      a   \n",
       "4                         <|begin_of_text|>I've had a                    few   \n",
       "5               <|begin_of_text|>I've had a perfectly                   good   \n",
       "6     <|begin_of_text|>I've had a perfectly wonderful                   life   \n",
       "7   <|begin_of_text|>I've had a perfectly wonderfu...                   with   \n",
       "8   <|begin_of_text|>I've had a perfectly wonderfu...                    and   \n",
       "9   <|begin_of_text|>I've had a perfectly wonderfu...                      I   \n",
       "10  <|begin_of_text|>I've had a perfectly wonderfu...                morning   \n",
       "11  <|begin_of_text|>I've had a perfectly wonderfu...                     't   \n",
       "12  <|begin_of_text|>I've had a perfectly wonderfu...                     it   \n",
       "\n",
       "   Prob of Most Likely Correct Next Token Prob of Correct Token  \\\n",
       "0               0.3013                  I                0.0069   \n",
       "1               0.0934                've                0.0297   \n",
       "2               0.3544                had                0.0495   \n",
       "3               0.3027                  a                0.3027   \n",
       "4               0.1439          perfectly                0.0001   \n",
       "5               0.1742          wonderful                0.0383   \n",
       "6               0.1588            evening                0.0093   \n",
       "7               0.1649                  ,                0.1101   \n",
       "8               0.1038                but                0.1030   \n",
       "9               0.3272               this                0.0179   \n",
       "10              0.4198               wasn                0.0016   \n",
       "11              0.9970                 't                0.9970   \n",
       "12              0.2101                 it                0.2101   \n",
       "\n",
       "   Negative Log Prob  \n",
       "0             4.9784  \n",
       "1             3.5150  \n",
       "2             3.0060  \n",
       "3             1.1951  \n",
       "4             9.2520  \n",
       "5             3.2614  \n",
       "6             4.6765  \n",
       "7             2.2059  \n",
       "8             2.2735  \n",
       "9             4.0239  \n",
       "10            6.4166  \n",
       "11            0.0030  \n",
       "12            1.5600  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1698d58b-13fe-46d9-9897-ca0fef64b93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.5667153846153847)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_numeric(df['Negative Log Prob']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62306fd7-a531-4234-bae3-b444b771aa72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
